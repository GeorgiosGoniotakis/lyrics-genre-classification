Title: Fully Connected - Words: 3000, Batch: 256, Epochs: 10, Outputs: 512

Results of Training Set

--------------------
Accuracy: 0.6677688472886357, F1 Score: 0.6506037011667989, Precision: 0.6780978031305103, Recall: 0.6425671660696076

              precision    recall  f1-score   support

           0       0.67      0.82      0.74     13170
           1       0.75      0.56      0.64      6048
           2       0.71      0.42      0.52      7020
           3       0.58      0.59      0.58     10425
           4       0.68      0.67      0.68      7642
           5       0.68      0.80      0.74     11777
           6       0.62      0.71      0.66     18547
           7       0.76      0.78      0.77     12692
           8       0.64      0.44      0.52     10969

   micro avg       0.67      0.67      0.67     98290
   macro avg       0.68      0.64      0.65     98290
weighted avg       0.67      0.67      0.66     98290
 samples avg       0.67      0.67      0.67     98290


[[10805    55   189   294   310   169   735   299   314]
 [  183  3364    75   485   330   416   584   427   184]
 [  898    97  2920   895   376   374   956    84   420]
 [  647   208   233  6167   302   588  1591   207   482]
 [  424   190   106   289  5155   253   677   346   202]
 [  360   119    87   411   133  9435   595   246   391]
 [ 1005   177   247  1073   444   705 13089  1264   543]
 [  160   105    22   168   125   912  1126  9907   167]
 [ 1638   150   232   904   352   950  1764   186  4793]]

Results of Validation Set

--------------------
Accuracy: 0.4139730793883344, F1 Score: 0.3754757798641442, Precision: 0.38417378337133606, Recall: 0.38178711400581794

              precision    recall  f1-score   support

           0       0.50      0.64      0.56      4336
           1       0.38      0.24      0.30      1980
           2       0.26      0.13      0.17      2305
           3       0.29      0.29      0.29      3499
           4       0.40      0.39      0.39      2578
           5       0.50      0.63      0.56      3966
           6       0.37      0.43      0.40      6032
           7       0.52      0.55      0.54      4331
           8       0.23      0.14      0.18      3736

   micro avg       0.41      0.41      0.41     32763
   macro avg       0.38      0.38      0.38     32763
weighted avg       0.39      0.41      0.40     32763
 samples avg       0.41      0.41      0.41     32763


[[2788   30  136  169  212  126  434  177  264]
 [  87  477   48  238  229  241  272  281  107]
 [ 449   67  293  428  179  226  432   49  182]
 [ 356  117  212 1011  169  343  860   99  332]
 [ 282  146   77  164  993  147  388  274  107]
 [ 160   94   60  253   79 2501  361  209  249]
 [ 514  127  161  598  271  370 2596  966  429]
 [ 131   97   12   80  181  491  883 2375   81]
 [ 839   97  147  519  154  508  849   94  529]]

Results of Test Set

--------------------
Accuracy: 0.4129837626663411, F1 Score: 0.37489025380092705, Precision: 0.38401304292326693, Recall: 0.38006213271899003

              precision    recall  f1-score   support

           0       0.50      0.62      0.55      4490
           1       0.38      0.25      0.30      1910
           2       0.26      0.13      0.17      2407
           3       0.27      0.28      0.28      3403
           4       0.39      0.37      0.38      2559
           5       0.51      0.63      0.56      3854
           6       0.38      0.44      0.41      6184
           7       0.51      0.54      0.52      4284
           8       0.25      0.16      0.19      3673

   micro avg       0.41      0.41      0.41     32764
   macro avg       0.38      0.38      0.37     32764
weighted avg       0.40      0.41      0.40     32764
 samples avg       0.41      0.41      0.41     32764


[[2779   33  159  210  181  138  499  200  291]
 [  81  472   45  238  237  190  280  244  123]
 [ 419   74  311  488  192  200  473   36  214]
 [ 356  122  228  969  178  330  816  122  282]
 [ 246  164   73  145  946  158  423  290  114]
 [ 174   78   83  220   81 2444  345  185  244]
 [ 522  135  144  655  265  363 2719 1041  340]
 [ 141   83   17   98  171  509  858 2319   88]
 [ 810   77  129  503  180  471  816  115  572]]

